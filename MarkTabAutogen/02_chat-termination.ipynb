{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Terminating Conversations Between Agents\n\nIn this chapter, we will explore how to terminate a conversation between AutoGen agents.\n\n_But why is this important?_ Its because in any complex, autonomous workflows it's crucial to know when to stop the workflow. For example, when the task is completed, or perhaps when the process has consumed enough resources and needs to either stop or adopt different strategies, such as user intervention. So AutoGen natively supports several mechanisms to terminate conversations.\n\nHow to Control Termination with AutoGen?\nCurrently there are two broad mechanism to control the termination of conversations between agents:\n\n1. **Specify parameters in `initiate_chat`**: When initiating a chat, you can define parameters that determine when the conversation should end.\n\n2. **Configure an agent to trigger termination**: When defining individual agents, you can specify parameters that allow agents to terminate of a conversation based on particular (configurable) conditions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Parameters in `initiate_chat`\n",
        "In the previous chapter we actually demonstrated this when we used the `max_turns` parameter to limit the number of turns. If we increase `max_turns` to say `3` notice the conversation takes more rounds to terminate:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('credentials.env') \n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\":  os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'],\n",
        "        \"api_key\": os.environ['AZURE_OPENAI_API_KEY'],\n",
        "        \"azure_endpoint\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
        "        \"api_type\": \"azure\",\n",
        "        \"api_version\": \"2024-02-15-preview\"\n",
        "    }\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1730904991182
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cathy = ConversableAgent(\n",
        "    \"cathy\",\n",
        "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": None,\n",
        "        \"config_list\": config_list,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.9\n",
        "    },\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": None,\n",
        "        \"config_list\": config_list,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1730904991291
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mjoe\u001b[0m (to cathy):\n\nCathy, tell me a joke.\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nSure, Joe! Why don't scientists trust atoms?\n\nBecause they make up everything!\n\n--------------------------------------------------------------------------------\n\u001b[33mjoe\u001b[0m (to cathy):\n\nGood one, Cathy! I've got one for you. Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nNice one, Joe! That’s a classic! Here’s another for you: Why did the bicycle fall over?\n\nBecause it was two-tired!\n\n--------------------------------------------------------------------------------\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1730904997507
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = joe.initiate_chat(\n",
        "    cathy, message=\"Cathy, tell me a joke.\", max_turns=3\n",
        ")  # increase the number of max turns before termination"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mjoe\u001b[0m (to cathy):\n\nCathy, tell me a joke.\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nSure, Joe! Why don't scientists trust atoms?\n\nBecause they make up everything!\n\n--------------------------------------------------------------------------------\n\u001b[33mjoe\u001b[0m (to cathy):\n\nHaha, that's a good one, Cathy! Speaking of making things up, remember that time we tried to improvise a whole comedy skit on the spot?\n\nCathy: Oh, how could I forget? It was a mix of brilliance and total disaster!\n\nJoe: Mostly disaster, but hey, at least we made people laugh!\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nCathy: Exactly! Remember the bit where you pretended to be a clueless tourist asking for directions?\n\nJoe: Oh, I was like, \"Excuse me, which way to the Eiffel Tower?\" and we were in the middle of New York City!\n\nCathy: And I said, \"Sure, just take a left at the Statue of Liberty, then a right at the Empire State Building, and if you see Central Park, you've gone too far!\"\n\nJoe: Ah, good times. We definitely confused a few folks in the audience with that one.\n\nCathy: Hey, as long as they were laughing, we were winning!\n\n--------------------------------------------------------------------------------\n\u001b[33mjoe\u001b[0m (to cathy):\n\n\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nJoe: Absolutely! And then there was that part where we did the impromptu song about lost socks.\n\nCathy: Oh yes, the \"Ballad of the Lonely Sock.\" \n\nJoe: It went something like, \"Where oh where could my partner be? I looked in the dryer, but it's just me!\"\n\nCathy: And then we had the chorus, \"Stuck in the spin cycle of life, just a lonely sock without a wife.\"\n\nJoe: That was such a hit! People still ask us to sing it at shows.\n\nCathy: Who knew laundry could be so inspirational?\n\nJoe: It's the little things in life, Cathy.\n\n--------------------------------------------------------------------------------\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:1461: UserWarning: Extracted_response from ChatCompletion(id='chatcmpl-AQbo68A6YFckVfmRZ5AIywWRpzgtt', choices=[Choice(finish_reason='content_filter', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}, logprobs=None)], created=1730905222, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_36b0c83da2', usage=CompletionUsage(completion_tokens=112, prompt_tokens=266, total_tokens=378), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], cost=0.0030099999999999997, message_retrieval_function=<bound method OpenAIClient.message_retrieval of <autogen.oai.client.OpenAIClient object at 0x7fbd807e43d0>>, config_id=0, pass_filter=True) is None.\n  warnings.warn(f\"Extracted_response from {response} is None.\", UserWarning)\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1730905225487
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent-triggered termination\n",
        "You can also terminate a conversation by configuring parameters of an agent.\n",
        "Currently, there are two parameters you can configure:\n",
        "\n",
        "1. `max_consecutive_auto_reply`: This condition triggers termination if the number of automatic responses to the same sender exceeds a threshold. You can customize this using the `max_consecutive_auto_reply` argument of the `ConversableAgent` class. To accomplish this the agent maintains a counter of the number of consecutive automatic responses to the same sender. Note that this counter can be reset because of human intervention. We will describe this in more detail in the next chapter.\n",
        "2. `is_termination_msg`: This condition can trigger termination if the _received_ message satisfies a particular condition, e.g., it contains the word \"TERMINATE\". You can customize this condition using the `is_terminate_msg` argument in the constructor of the `ConversableAgent` class."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `max_consecutive_auto_reply`\n",
        "\n",
        "In the example below lets set `max_consecutive_auto_reply` to `1` and notice how this ensures that Joe only replies once."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": None,\n",
        "        \"config_list\": config_list,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
        ")\n",
        "\n",
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mjoe\u001b[0m (to cathy):\n\nCathy, tell me a joke.\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nSure, Joe! Here's one for you:\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!\n\n--------------------------------------------------------------------------------\n\u001b[33mjoe\u001b[0m (to cathy):\n\nGood one, Cathy! Here’s one from me:\n\nWhy don’t skeletons fight each other?\n\nThey don’t have the guts!\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nCathy:\nGood one, Joe! You're really cracking me up. But seriously, those skeletons are just spineless!\n\n--------------------------------------------------------------------------------\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1730905292300
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `is_termination_msg`\n",
        "\n",
        "Let's set the termination message to \"GOOD BYE\" and see how the conversation terminates."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": None,\n",
        "        \"config_list\": config_list,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        "    is_termination_msg=lambda msg: \"good bye\" in msg[\"content\"].lower(),\n",
        ")\n",
        "\n",
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke and then say the words GOOD BYE.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mjoe\u001b[0m (to cathy):\n\nCathy, tell me a joke and then say the words GOOD BYE.\n\n--------------------------------------------------------------------------------\n\u001b[33mcathy\u001b[0m (to joe):\n\nSure thing, Joe! Here goes:\n\nWhy don’t scientists trust atoms?\n\nBecause they make up everything!\n\nGOOD BYE!\n\n--------------------------------------------------------------------------------\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1730905301454
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice how the conversation ended based on contents of cathy's message!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this chapter we introduced mechanisms to terminate a conversation between agents.\n",
        "You can configure both parameters in `initiate_chat` and also configuration of agents.\n",
        "\n",
        "That said, it is important to note that when a termination condition is triggered,\n",
        "the conversation may not always terminate immediately. The actual termination\n",
        "depends on the `human_input_mode` argument of the `ConversableAgent` class.\n",
        "For example, when mode is `NEVER` the termination conditions above will end the conversations.\n",
        "But when mode is `ALWAYS` or `TERMINATE`, it will not terminate immediately.\n",
        "We will describe this behavior and explain why it is important in the next chapter.\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
